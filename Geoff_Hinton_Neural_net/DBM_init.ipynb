{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing multi modal joint representation  for image captioning using  multimodal deep boltzman machine using tensorflow\n",
    "\n",
    "## here we will be using flickr dataset consisting of 10k pictures and text captions (somewhat obfuscated ) . \n",
    "\n",
    "### we will be implementing  two stacks  of dbm's ( for learning text and image  representation ) which are trained unsupervised way then combined usinng single layer\n",
    "\n",
    "For more Info , read the paper :- __[click](http://papers.nips.cc/paper/4683-multimodal-learning-with-deep-boltzmann-machines.pdf)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. implementing the DBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "#these are from files attached from repo\n",
    "\n",
    "from unsupervised_model import UnsupervisedModel\n",
    "import rbm\n",
    "\n",
    "\n",
    "#pip install yadlt\n",
    "from yadlt.utils import utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now for DBM , we will be using following parameters\n",
    "\n",
    "### param layers:- list containing the hidden units for each layer\n",
    "### param finetune_loss_func: Loss function for the softmax layer. string, default ['cross_entropy', 'mean_squared']\n",
    "        \n",
    "### :param finetune_dropout: dropout parameter\n",
    "### :param finetune_learning_rate: learning rate for the finetuning. float, default 0.001\n",
    "  ### :param finetune_enc_act_func: activation function for the encoder\n",
    "   ### finetuning phase\n",
    "        :param finetune_dec_act_func: activation function for the decoder\n",
    "            finetuning phase\n",
    "        :param finetune_opt: optimizer for the finetuning phase\n",
    "        :param finetune_num_epochs: Number of epochs for the finetuning.\n",
    "            int, default 20\n",
    "        :param finetune_batch_size: Size of each mini-batch for the finetuning.\n",
    "            int, default 20\n",
    "        :param verbose: Level of verbosity. 0 - silent, 1 - print accuracy.\n",
    "            int, default 0\n",
    "        :param do_pretrain: True: uses variables from pretraining,\n",
    "            False: initialize new variables.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementing the dbm \n",
    "class DBM(UnsupervisedModel):\n",
    "    # initializing the parameters in constructor (although they are too much but it is  not req as we can  get it implemented in tensorflow)\n",
    "      def __init__(\n",
    "        self, layers, model_name='srbm', main_dir='srbm/',\n",
    "        models_dir='models/', data_dir='data/', summary_dir='logs/',\n",
    "        num_epochs=[10], batch_size=[10], dataset='mnist',\n",
    "        learning_rate=[0.01], gibbs_k=[1], loss_func=['mean_squared'],\n",
    "        momentum=0.5, finetune_dropout=1, verbose=1,\n",
    "        finetune_loss_func='cross_entropy', finetune_enc_act_func=[tf.nn.relu],\n",
    "        finetune_dec_act_func=[tf.nn.sigmoid], finetune_opt='gradient_descent',\n",
    "        finetune_learning_rate=0.001, l2reg=5e-4, finetune_num_epochs=10,\n",
    "        noise=['gauss'], stddev=0.1, finetune_batch_size=20, do_pretrain=False,\n",
    "        tied_weights=False, regtype=['none'], finetune_reg_type='none'):\n",
    "        expanded_args = utilities.expand_args(**locals())\n",
    "\n",
    "           # initializing the Unsupervised model in constructor\n",
    "    \n",
    "        UnsupervisedModel.__init__(\n",
    "        self, model_name, main_dir, models_dir, data_dir, summary_dir)    \n",
    "        self._initialize_training_parameters(\n",
    "            loss_func=finetune_loss_func, learning_rate=finetune_learning_rate,\n",
    "            regtype=finetune_reg_type, num_epochs=finetune_num_epochs,\n",
    "            batch_size=finetune_batch_size, l2reg=l2reg,\n",
    "            dropout=finetune_dropout, dataset=dataset, opt=finetune_opt,\n",
    "            momentum=momentum)\n",
    "\n",
    "        self.do_pretrain = do_pretrain\n",
    "        self.layers = layers\n",
    "        self.tied_weights = tied_weights\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.finetune_enc_act_func = expanded_args['finetune_enc_act_func']\n",
    "        self.finetune_dec_act_func = expanded_args['finetune_dec_act_func']\n",
    "\n",
    "        self.input_ref = None\n",
    "\n",
    "        # Model parameters\n",
    "        self.encoding_w_ = []  # list of matrices of encoding weights per layer\n",
    "        self.encoding_b_ = []  # list of arrays of encoding biases per layer\n",
    "\n",
    "        self.decoding_w = []  # list of matrices of decoding weights per layer\n",
    "        self.decoding_b = []  # list of arrays of decoding biases per layer\n",
    "\n",
    "        self.reconstruction = None\n",
    "        self.rbms = []\n",
    "        self.rbm_graphs = []\n",
    "\n",
    "        \n",
    "        # for initializing the weights and traning\n",
    "        \n",
    "        for l, layer in enumerate(layers):\n",
    "            rbm_str = 'rbm-' + str(l + 1)\n",
    "            new_rbm = rbm.RBM(\n",
    "                model_name=self.model_name + '-' + rbm_str,\n",
    "                loss_func=expanded_args['loss_func'][l],\n",
    "                models_dir=os.path.join(self.models_dir, rbm_str),\n",
    "                data_dir=os.path.join(self.data_dir, rbm_str),\n",
    "                summary_dir=os.path.join(self.tf_summary_dir, rbm_str),\n",
    "                visible_unit_type=expanded_args['noise'][l], stddev=stddev,\n",
    "                num_hidden=expanded_args['layers'][l], main_dir=self.main_dir,\n",
    "                learning_rate=expanded_args['learning_rate'][l],\n",
    "                gibbs_sampling_steps=expanded_args['gibbs_k'][l],\n",
    "                num_epochs=expanded_args['num_epochs'][l],\n",
    "                batch_size=expanded_args['batch_size'][l],\n",
    "                verbose=self.verbose, regtype=expanded_args['regtype'][l])\n",
    "            self.rbms.append(new_rbm)\n",
    "            self.rbm_graphs.append(tf.Graph())\n",
    "\n",
    "   \n",
    "\n",
    " #Perform Unsupervised pretraining of the autoencoder.\n",
    "       \n",
    "\n",
    "    def pretrain(self, train_set, validation_set=None):\n",
    "        self.do_pretrain = True\n",
    "\n",
    "        def set_params_func(rbmmachine, rbmgraph):\n",
    "            params = rbmmachine.get_model_parameters(graph=rbmgraph)\n",
    "            self.encoding_w_.append(params['W'])\n",
    "            self.encoding_b_.append(params['bh_'])\n",
    "\n",
    "        return UnsupervisedModel.pretrain_procedure(\n",
    "            self, self.rbms, self.rbm_graphs, set_params_func=set_params_func,\n",
    "            train_set=train_set, validation_set=validation_set)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "        \n",
    "        \n",
    "        \n",
    "        \"\"\"  now for Training  the model.\n",
    "        :param train_set: training set\n",
    "        :param train_ref: training reference data\n",
    "        :param validation_set: validation set\n",
    "        :param validation_ref: validation reference data\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "        def _train_model(self, train_set, train_ref,\n",
    "                     validation_set, validation_ref):\n",
    "       \n",
    "        shuff = zip(train_set, train_ref)\n",
    "\n",
    "        for i in range(self.num_epochs):\n",
    "\n",
    "            np.random.shuffle(shuff)\n",
    "            batches = [_ for _ in utilities.gen_batches(\n",
    "                shuff, self.batch_size)]\n",
    "\n",
    "            for batch in batches:\n",
    "                x_batch, y_batch = zip(*batch)\n",
    "                self.tf_session.run(\n",
    "                    self.train_step,\n",
    "                    feed_dict={self.input_data: x_batch,\n",
    "                               self.input_labels: y_batch,\n",
    "                               self.keep_prob: self.dropout})\n",
    "\n",
    "            if validation_set is not None:\n",
    "                feed = {self.input_data: validation_set,\n",
    "                        self.input_labels: validation_ref,\n",
    "                        self.keep_prob: 1}\n",
    "                self._run_validation_error_and_summaries(i, feed)\n",
    "\n",
    "    def build_model(self, n_features, regtype='none',\n",
    "                    encoding_w=None, encoding_b=None):\n",
    "        \"\"\"Create the computational graph for the reconstruction task.\n",
    "        :param n_features: Number of features\n",
    "        :param regtype: regularization type\n",
    "        :param encoding_w: list of weights for the encoding layers.\n",
    "        :param encoding_b: list of biases for the encoding layers.\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self._create_placeholders(n_features, n_features)\n",
    "\n",
    "        if encoding_w and encoding_b:\n",
    "            self.encoding_w_ = encoding_w\n",
    "            self.encoding_b_ = encoding_b\n",
    "        else:\n",
    "            self._create_variables(n_features)\n",
    "\n",
    "        self._create_encoding_layers()\n",
    "        self._create_decoding_layers()\n",
    "\n",
    "        vars = []\n",
    "        vars.extend(self.encoding_w_)\n",
    "        vars.extend(self.encoding_b_)\n",
    "        regterm = self.compute_regularization(vars)\n",
    "\n",
    "        self._create_cost_function_node(\n",
    "            self.reconstruction, self.input_labels, regterm=regterm)\n",
    "        self._create_train_step_node()\n",
    "\n",
    "    def _create_placeholders(self, n_features, n_classes):\n",
    "        \"\"\"Create the TensorFlow placeholders for the model.\n",
    "        :param n_features: number of features of the first layer\n",
    "        :param n_classes: number of classes\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.input_data = tf.placeholder(\n",
    "            tf.float32, [None, n_features], name='x-input')\n",
    "        self.input_labels = tf.placeholder(\n",
    "            tf.float32, [None, n_classes], name='y-input')\n",
    "        self.keep_prob = tf.placeholder(\n",
    "            tf.float32, name='keep-probs')\n",
    "\n",
    "    def _create_variables(self, n_features):\n",
    "        \"\"\"Create the TensorFlow variables for the model.\n",
    "        :param n_features: number of features\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        if self.do_pretrain:\n",
    "            self._create_variables_pretrain()\n",
    "        else:\n",
    "            self._create_variables_no_pretrain(n_features)\n",
    "\n",
    "    def _create_variables_no_pretrain(self, n_features):\n",
    "        \"\"\"Create model variables (no previous unsupervised pretraining).\n",
    "        :param n_features: number of features\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        self.encoding_w_ = []\n",
    "        self.encoding_b_ = []\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "\n",
    "            if l == 0:\n",
    "                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n",
    "                    shape=[n_features, self.layers[l]], stddev=0.1)))\n",
    "                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n",
    "                    [self.layers[l]], stddev=0.1)))\n",
    "            else:\n",
    "                self.encoding_w_.append(tf.Variable(tf.truncated_normal(\n",
    "                    shape=[self.layers[l - 1], self.layers[l]], stddev=0.1)))\n",
    "                self.encoding_b_.append(tf.Variable(tf.truncated_normal(\n",
    "                    [self.layers[l]], stddev=0.1)))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"   Create model variables  from previous unsupervised pretraining\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "    def _create_variables_pretrain(self):\n",
    "        \n",
    "        for l, layer in enumerate(self.layers):\n",
    "            self.encoding_w_[l] = tf.Variable(\n",
    "                self.encoding_w_[l], name='enc-w-{}'.format(l))\n",
    "            self.encoding_b_[l] = tf.Variable(\n",
    "                self.encoding_b_[l], name='enc-b-{}'.format(l))\n",
    "\n",
    "    def _create_encoding_layers(self):\n",
    "        \"\"\"Create the encoding layers for supervised finetuning.\n",
    "        :return: output of the final encoding layer.\n",
    "        \"\"\"\n",
    "        next_train = self.input_data\n",
    "        self.layer_nodes = []\n",
    "\n",
    "        for l, layer in enumerate(self.layers):\n",
    "\n",
    "            with tf.name_scope(\"encode-{}\".format(l)):\n",
    "\n",
    "                y_act = tf.add(\n",
    "                    tf.matmul(next_train, self.encoding_w_[l]),\n",
    "                    self.encoding_b_[l]\n",
    "                )\n",
    "\n",
    "                if self.finetune_enc_act_func[l] is not None:\n",
    "                    layer_y = self.finetune_enc_act_func[l](y_act)\n",
    "\n",
    "                else:\n",
    "                    layer_y = None\n",
    "\n",
    "                # the input to the next layer is the output of this layer\n",
    "                next_train = tf.nn.dropout(layer_y, self.keep_prob)\n",
    "\n",
    "            self.layer_nodes.append(next_train)\n",
    "\n",
    "        self.encode = next_train\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "     \"\"\"Create the decoding layers for reconstruction finetuning.\n",
    "        :returning value: output of the final encoding layer.\n",
    "        \"\"\"\n",
    "    def _create_decoding_layers(self):\n",
    "       \n",
    "        next_decode = self.encode\n",
    "\n",
    "        for l, layer in reversed(list(enumerate(self.layers))):\n",
    "\n",
    "            with tf.name_scope(\"decode-{}\".format(l)):\n",
    "\n",
    "                # Create decoding variables\n",
    "                if self.tied_weights:\n",
    "                    dec_w = tf.transpose(self.encoding_w_[l])\n",
    "                else:\n",
    "                    dec_w = tf.Variable(tf.transpose(\n",
    "                        self.encoding_w_[l].initialized_value()))\n",
    "\n",
    "                dec_b = tf.Variable(tf.constant(\n",
    "                    0.1, shape=[dec_w.get_shape().dims[1].value]))\n",
    "                self.decoding_w.append(dec_w)\n",
    "                self.decoding_b.append(dec_b)\n",
    "\n",
    "                y_act = tf.add(\n",
    "                    tf.matmul(next_decode, dec_w),\n",
    "                    dec_b\n",
    "                )\n",
    "\n",
    "                if self.finetune_dec_act_func[l] is not None:\n",
    "                    layer_y = self.finetune_dec_act_func[l](y_act)\n",
    "\n",
    "                else:\n",
    "                    layer_y = None\n",
    "\n",
    "                # the input to the next layer is the output of this layer\n",
    "                next_decode = tf.nn.dropout(layer_y, self.keep_prob)\n",
    "\n",
    "            self.layer_nodes.append(next_decode)\n",
    "\n",
    "self.reconstruction = next_decode\n",
    "            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
